{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d59a93-39f4-47c6-86c7-5dbfde890d17",
   "metadata": {},
   "source": [
    "Name: Holly Kular\\\n",
    "Date: 03-19-2024\\\n",
    "Email: hkular@ucsd.edu\\\n",
    "decode_L1.m\\\n",
    "Description: Script for decoding analysis on layer 1 of probabilistic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f6a786-a74f-4995-9f98-ccdbe0e06f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.io import loadmat\n",
    "from fnc_fit_and_score import fnc_fit_and_score\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b482eec4-0546-4a55-9444-b34caf490694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(elements, window_size):\n",
    "  if len(elements) <= window_size:\n",
    "    return elements\n",
    "\n",
    "  windows = []\n",
    "  for i in range(len(elements) - window_size + 1):\n",
    "    windows.append(elements[i:i + window_size])\n",
    "\n",
    "  return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "577c14b9-d627-4924-b61a-254605a0d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY HERE\n",
    "# what conditions were the RNNs trained on?\n",
    "RNN_params = {\n",
    "    'prob_split': '70_30',\n",
    "    'afc': [6, 2],\n",
    "    'coh': ['hi', 'lo'],\n",
    "    'feedback': False,\n",
    "    'thresh': [.3, .7],\n",
    "    'model': 2\n",
    "}\n",
    "\n",
    "D_params = {\n",
    "    'time_avg': False,\n",
    "    't_win': [130, -1],\n",
    "    'n_cvs': 5,\n",
    "    'num_cgs': 30,\n",
    "    'label': ['stim', 'choice'],  # 'stim' or 'choice'\n",
    "    'units': 'all',  # 'all' or 'exc' or 'inh'\n",
    "    'pred': 'all'  # 'expected' or 'unexpected', 'all'\n",
    "}\n",
    "# Timing of task\n",
    "task_info = {\n",
    "    'trials': 1000,\n",
    "    'trial_dur': 250,\n",
    "    'stim_on': 80,\n",
    "    'stim_dur': 50\n",
    "}\n",
    "n_cvs = 5\n",
    "window = 50\n",
    "# penalties to eval\n",
    "num_cgs = 30\n",
    "Cs = np.logspace( -5,1,num_cgs )\n",
    "\n",
    "# set up the grid\n",
    "param_grid = { 'C': Cs, 'kernel': ['linear'] }\n",
    "\n",
    "# define object - use a SVC that balances class weights (because they are biased, e.g. 70/30)\n",
    "# note that can also specify cv folds here, but I'm doing it by hand below in a loop\n",
    "grid = GridSearchCV( SVC(class_weight = 'balanced'),param_grid,refit=True,verbose=0 )\n",
    "\n",
    "modelnum = RNN_params['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe0f31-71ed-4292-9d3e-a8cb5ba18c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done decoding\n",
      "done decoding\n"
     ]
    }
   ],
   "source": [
    "combinations = list(itertools.product(RNN_params['afc'], RNN_params['coh'], D_params['label']))\n",
    "\n",
    "for afc, coh, label in combinations:\n",
    "    # Load data\n",
    "    if sys.platform.startswith('linux'):\n",
    "        data_dir = f\"/mnt/neurocube/local/serenceslab/holly/RNN_Geo/data/rdk_{RNN_params['prob_split']}_{afc}afc/feedforward_only/{coh}_coh\"\n",
    "    else:\n",
    "        data_dir = f\"/Volumes/serenceslab/holly/RNN_Geo/data/rdk_{RNN_params['prob_split']}_{afc}afc/feedforward_only/{coh}_coh\"\n",
    "    \n",
    "    # Chose the model\n",
    "    mat_files = [f for f in os.listdir(data_dir) if f.endswith('.mat')]# Get all the trained models (should be 40 .mat files)\n",
    "    model_path = os.path.join(data_dir, mat_files[RNN_params['model']]) \n",
    "    model = loadmat(model_path) # model.keys()\n",
    "    \n",
    "    # get the data from layer 1 decode stim\n",
    "    # this is a [trial x time step x unit] matrix\n",
    "    data_file = f\"{data_dir}/Trials{task_info['trials']}_model{model_path[-7:-4]}_balanced.npz\"\n",
    "    data = np.load(data_file)\n",
    "    data_d = data['fr1']\n",
    "    if label == 'stim':\n",
    "        labs = data['labs'].squeeze()\n",
    "    elif label == 'choice':\n",
    "        labs = data['outs'][:,-1]\n",
    "    \n",
    "    # get some info about structure of the data\n",
    "    tris = data_d.shape[0]             # number of trials\n",
    "    tri_ind = np.arange(0,tris)      # list from 0...tris\n",
    "    hold_out = int( tris / n_cvs )   # how many trials to hold out\n",
    "    thresh = RNN_params.get('thresh', [.3, .7])\n",
    "    if label == 'stim':\n",
    "        n_classes = len(np.unique(labs))\n",
    "    else:\n",
    "        n_classes = 2\n",
    "    if D_params['time_avg'] :\n",
    "        data_d = np.mean( data_d[ :,D_params['t_win'][0]:D_params['t_win'][1],: ], axis = 1 ) # average over time window\n",
    "        acc = np.zeros(n_cvs)\n",
    "        cm = np.zeros((n_cvs, n_classes, n_classes))\n",
    "        class_acc = np.zeros((n_cvs, n_classes))\n",
    "        # Within each cross-validation fold\n",
    "        for i in range(n_cvs):\n",
    "    \n",
    "            # trials to hold out as test set on this cv fold\n",
    "            tst_ind = tri_ind[ i*hold_out : (i+1)*hold_out ]\n",
    "    \n",
    "            # index into the training data on this cv fold\n",
    "            trn_ind = np.setdiff1d( tri_ind, tst_ind )\n",
    "    \n",
    "            # get the training data (X) and the training labels (y)\n",
    "            X = data_d[ trn_ind,: ]\n",
    "            if label == 'stim':\n",
    "                y = labs[trn_ind]\n",
    "            else:\n",
    "                y = np.select([labs[trn_ind] >= thresh[1], labs[trn_ind] <= thresh[0]], [0,1], default=0)\n",
    "    \n",
    "    \n",
    "            # Fit the model on the binary labels\n",
    "            grid.fit( X, y )\n",
    "    \n",
    "            # get the test data (X) and the test labels (y)\n",
    "            X_test = data_d[tst_ind, :]\n",
    "            if label == 'stim':\n",
    "                y_test = labs[tst_ind]\n",
    "            else:\n",
    "                y_test = np.select([labs[tst_ind] >= thresh[1], labs[tst_ind] <= thresh[0]], [0,1], default=0)\n",
    "    \n",
    "    \n",
    "            # predict!\n",
    "            y_pred = grid.predict(X_test)\n",
    "            score = grid.score( X_test,y_test )\n",
    "            acc[i] += score  # Append accuracy for this CV fold\n",
    "            # confusion matrix\n",
    "            cm[i] += confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "            # Evaluate accuracy\n",
    "            accuracy = np.mean( acc )\n",
    "            for cls in range(n_classes):\n",
    "                cls_ind = y_test == cls\n",
    "                class_acc[i, cls] += (np.sum(y_pred[cls_ind] == cls) / np.sum(cls_ind))\n",
    "                \n",
    "            \n",
    "            # Print overall results\n",
    "            #print(f'CV: {i}, {grid.best_estimator_}')\n",
    "        print(f'done decoding')\n",
    "        cm_mean = np.mean(cm, axis = 0)\n",
    "        exec(f'{coh}_{afc}_{label}_all{model}_avg = cm_mean')\n",
    "    else:\n",
    "    # Do decoding - parallel \n",
    "        times = sliding_window(range(task_info['stim_dur']+task_info['stim_on'],task_info['trial_dur']), window)\n",
    "        pool = Pool(processes=round(os.cpu_count() * .8))\n",
    "        with pool:  # use 70% of cpus\n",
    "            results = pool.starmap(fnc_fit_and_score, [\n",
    "                (t, np.mean( data_d[:,t, :], axis = 1 ), tri_ind, hold_out, n_cvs, n_classes, labs, label, thresh, grid)\n",
    "                for t in times\n",
    "            ], chunksize = 10)\n",
    "        pool.close()\n",
    "        # Process the results from each worker process (list of lists of accuracies)\n",
    "        #decoding_acc = np.mean(np.array(results), axis=1)\n",
    "        print(f'done decoding')\n",
    "        acc = np.array(results)    \n",
    "        exec(f'{coh}_{afc}_{label}_all{modelnum} = acc')\n",
    "print(f'done saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288edc54-5b5c-4189-81db-a55ea9c24e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "full_file = f'/mnt/neurocube/local/serenceslab/holly/RNN_Geo/data/decoding/results_balanced_all{RNN_params['model']}.npz'\n",
    "np.savez(full_file, lo_2_stim_all2 = lo_2_stim_all2, lo_6_stim_all2 = lo_6_stim_all2, hi_2_stim_all2 = hi_2_stim_all2, hi_6_stim_all2 = hi_6_stim_all2, lo_2_choice_all2 = lo_2_choice_all2, lo_6_choice_all2 = lo_6_choice_all2, hi_2_choice_all2 = hi_2_choice_all2, hi_6_choice_all2 = hi_6_choice_all2)\n",
    "print(f'done saving')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
